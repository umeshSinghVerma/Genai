{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H972ujuTsXp1",
        "outputId": "24cc3cf1-1f97-451d-d629-715d115a9003"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install pymilvus\n",
        "!pip install pytesseract\n",
        "!pip install langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rpAw81hKMIQm",
        "outputId": "cfab9976-101e-49c0-9ea2-6ada6c9933c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping: https://www.zomato.com/lucknow/punjab-grill-gomti-nagar/order\n",
            "Found 88 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/royal-cafe-royal-inn-sapru-marg/order\n",
            "Found 208 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/barkaas-indo-arabic-restaurant-1-aliganj/order\n",
            "Found 170 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/hazratganj-social-hazratganj/order\n",
            "Found 174 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/cafe-hons-house-of-no-sugar-gomti-nagar/order\n",
            "Found 181 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/kake-da-hotel-since-1931-jankipuram/order\n",
            "Found 176 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/cafe-delhi-heights-sadar-bazaar/order\n",
            "Found 201 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/mcdonalds-2-hazratganj/order\n",
            "Found 205 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/grand-patio-hotel-savvy-grand-gomti-nagar/order\n",
            "Found 99 menu items.\n",
            "Scraping: https://www.zomato.com/lucknow/abongzaa-multi-cuisine-cafe-restaurant-gomti-nagar/order\n",
            "Found 171 menu items.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# === Configuration ===\n",
        "OUTPUT_DIR = \"./zomato_scraped_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "URLS = [\n",
        "    \"https://www.zomato.com/lucknow/punjab-grill-gomti-nagar/order\",\n",
        "    \"https://www.zomato.com/lucknow/royal-cafe-royal-inn-sapru-marg/order\",\n",
        "    \"https://www.zomato.com/lucknow/barkaas-indo-arabic-restaurant-1-aliganj/order\",\n",
        "    \"https://www.zomato.com/lucknow/hazratganj-social-hazratganj/order\",\n",
        "    \"https://www.zomato.com/lucknow/cafe-hons-house-of-no-sugar-gomti-nagar/order\",\n",
        "    \"https://www.zomato.com/lucknow/kake-da-hotel-since-1931-jankipuram/order\",\n",
        "    \"https://www.zomato.com/lucknow/cafe-delhi-heights-sadar-bazaar/order\",\n",
        "    \"https://www.zomato.com/lucknow/mcdonalds-2-hazratganj/order\",\n",
        "    \"https://www.zomato.com/lucknow/grand-patio-hotel-savvy-grand-gomti-nagar/order\",\n",
        "    \"https://www.zomato.com/lucknow/abongzaa-multi-cuisine-cafe-restaurant-gomti-nagar/order\"\n",
        "]\n",
        "\n",
        "# Setup Chrome for Selenium\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--remote-debugging-port=9222')\n",
        "chrome_options.add_argument(\n",
        "    \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\"\n",
        ")\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "def create_page_folder(title):\n",
        "    folder_name = re.sub(r'[^a-zA-Z0-9]+', '_', title).strip('_')\n",
        "    folder_path = os.path.join(OUTPUT_DIR, folder_name)\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    return folder_path\n",
        "\n",
        "def save_file(content, path, binary=False):\n",
        "    mode = 'wb' if binary else 'w'\n",
        "    with open(path, mode, encoding=None if binary else 'utf-8') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def extract_markdown(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup.get_text(separator='\\n')\n",
        "\n",
        "def extract_structured_data(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Extract <script type=\"application/ld+json\"> data\n",
        "    scripts = soup.find_all('script', type='application/ld+json')\n",
        "    restaurant_data = None\n",
        "    for script in scripts:\n",
        "        try:\n",
        "            data = json.loads(script.string.strip())\n",
        "            if isinstance(data, list):\n",
        "                for d in data:\n",
        "                    if d.get('@type') == 'Restaurant':\n",
        "                        restaurant_data = d\n",
        "            elif data.get('@type') == 'Restaurant':\n",
        "                restaurant_data = data\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing ld+json:\", e)\n",
        "\n",
        "    # Extract menu items from the page\n",
        "    menu_items = []\n",
        "    menu_divs = soup.find_all(\"div\", class_=\"sc-iAVDmT bWpTfk\")  # Use the correct class for menu card\n",
        "\n",
        "    print(f\"Found {len(menu_divs)} menu items.\")\n",
        "\n",
        "    for item in menu_divs:\n",
        "        try:\n",
        "            # Extract menu title (name of the dish)\n",
        "            name_tag = item.find(\"h4\", class_=\"sc-cGCqpu chKhYc\")\n",
        "            name = name_tag.text.strip() if name_tag else \"No name available\"\n",
        "\n",
        "            # Extract bestseller status (True or False)\n",
        "            bestseller_tag = item.find(\"div\", class_=\"sc-2gamf4-0 fSJGVb\")\n",
        "            is_bestseller = \"BESTSELLER\" in bestseller_tag.text.upper() if bestseller_tag else False\n",
        "\n",
        "            # Extract veg/non-veg type from div\n",
        "            dish_type = \"unknown\"\n",
        "            type_svg = item.find('svg', class_=\"sc-eOnLuU dlDRKy\")  # Using the primary class\n",
        "            if type_svg:\n",
        "              svg_str = str(type_svg)\n",
        "              if \"#veg-icon\" in svg_str:\n",
        "                  dish_type = \"veg\"\n",
        "              elif \"#non-veg-icon\" in svg_str:\n",
        "                  dish_type = \"non-veg\"\n",
        "\n",
        "\n",
        "\n",
        "            # Extract price\n",
        "            price_tag = item.find(\"span\", class_=\"sc-17hyc2s-1 cCiQWA\")\n",
        "            price_text = price_tag.text.strip().replace(\"₹\", \"\") if price_tag else \"0\"\n",
        "            price = int(re.sub(r\"[^\\d]\", \"\", price_text)) if price_text else 0\n",
        "\n",
        "            # Extract description\n",
        "            desc_tag = item.find(\"p\", class_=\"sc-gsxalj jqiNmO\")\n",
        "            description = desc_tag.text.strip() if desc_tag else \"No description available\"\n",
        "\n",
        "            menu_items.append({\n",
        "                \"name\": name,\n",
        "                \"isBestseller\": is_bestseller,\n",
        "                \"price\": price,\n",
        "                \"priceCurrency\": \"INR\",\n",
        "                \"description\": description,\n",
        "                \"isVeg\": dish_type,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(\"Error extracting item:\", e)\n",
        "\n",
        "    menu_data = {\n",
        "        \"@type\": \"Menu\",\n",
        "        \"hasMenuItem\": menu_items\n",
        "    }\n",
        "\n",
        "    # If no menu items were found, log that the extraction failed\n",
        "    if not menu_items:\n",
        "        print(\"No menu items were extracted.\")\n",
        "\n",
        "    return {\n",
        "        \"restaurant\": restaurant_data,\n",
        "        \"menu\": menu_data\n",
        "    }\n",
        "\n",
        "\n",
        "def scrape_page(url):\n",
        "    print(f\"Scraping: {url}\")\n",
        "    driver.get(url)\n",
        "    time.sleep(5)  # wait for page to load JS content\n",
        "\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    title = soup.title.string.strip() if soup.title else 'untitled'\n",
        "    folder = create_page_folder(title)\n",
        "\n",
        "    # Save raw HTML\n",
        "    save_file(html, os.path.join(folder, 'page.html'))\n",
        "\n",
        "    # Save plain markdown-style content\n",
        "    md_content = extract_markdown(html)\n",
        "    save_file(md_content, os.path.join(folder, 'page.md'))\n",
        "\n",
        "    # Extract and save structured data\n",
        "    structured_data = extract_structured_data(html)\n",
        "    save_file(json.dumps(structured_data, indent=2), os.path.join(folder, 'structured_data.json'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for link in URLS:\n",
        "        try:\n",
        "            scrape_page(link)\n",
        "        except Exception as err:\n",
        "            print(f\"Error scraping {link}: {err}\")\n",
        "    driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N3wW3jH1i7zs",
        "outputId": "80b9c8dc-e6da-42ef-a525-2cb3450aad28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated ./zomato_scraped_data\\Abongzaa_Multi_Cuisine_Cafe_Restaurant_Gomti_Nagar_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Barkaas_Indo_Arabic_Restaurant_Aliganj_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Cafe_Delhi_Heights_Sadar_Bazaar_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Cafe_Hons_House_Of_No_Sugar_Gomti_Nagar_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Grand_Patio_Hotel_Savvy_Grand_Gomti_Nagar_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Hazratganj_SOCIAL_Hazratganj_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Kake_Da_Hotel_Since_1931_Jankipuram_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\McDonald_s_Hazratganj_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Punjab_Grill_Gomti_Nagar_order_online_Zomato\\structured_data.json\n",
            "Updated ./zomato_scraped_data\\Royal_Cafe_Royal_Inn_Sapru_Marg_order_online_Zomato\\structured_data.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "OUTPUT_DIR = \"./zomato_scraped_data\"\n",
        "\n",
        "def process_price_range():\n",
        "    # Iterate through each restaurant directory\n",
        "    for restaurant_dir in os.listdir(OUTPUT_DIR):\n",
        "        dir_path = os.path.join(OUTPUT_DIR, restaurant_dir)\n",
        "        json_path = os.path.join(dir_path, 'structured_data.json')\n",
        "\n",
        "        # Skip if the JSON file doesn't exist\n",
        "        if not os.path.exists(json_path):\n",
        "            continue\n",
        "\n",
        "        # Load existing data\n",
        "        try:\n",
        "            with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {json_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Extract menu items and prices\n",
        "        menu_items = data.get('menu', {}).get('hasMenuItem', [])\n",
        "        prices = []\n",
        "        for item in menu_items:\n",
        "            price = item.get('price', 0)\n",
        "            if isinstance(price, (int, float)) and price > 0:  # Only consider valid positive prices\n",
        "                prices.append(price)\n",
        "\n",
        "        # Calculate price range\n",
        "        price_range = \"\"\n",
        "        if prices:\n",
        "            min_price = min(prices)\n",
        "            max_price = max(prices)\n",
        "            price_range = f\"₹{min_price} - ₹{max_price}\"\n",
        "\n",
        "        # Update restaurant data\n",
        "        restaurant_data = data.get('restaurant', {})\n",
        "        if restaurant_data:\n",
        "            restaurant_data['priceRange'] = price_range\n",
        "            data['restaurant'] = restaurant_data\n",
        "\n",
        "        # Save updated data back to file\n",
        "        try:\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"Updated {json_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {json_path}: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_price_range()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dYDyTtnErCFC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import logging\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# === Configuration ===\n",
        "OUTPUT_DIR = os.getenv(\"ZOMATO_DATA_DIR\", \"./zomato_scraped_data\")\n",
        "\n",
        "# Pre-compile regex patterns once\n",
        "INGREDIENTS_PATTERNS = [\n",
        "    re.compile(r'(?:contains|made with|ingredients)[:\\s]*(.*?)(?:[.;]|$)', re.IGNORECASE),\n",
        "]\n",
        "SPICE_LEVEL_PATTERN = re.compile(r'spice level[:\\s]*([0-5])(?:/5)?', re.IGNORECASE)\n",
        "\n",
        "# Dietary and category configurations\n",
        "DIETARY_KEYWORDS: Dict[str, List[str]] = {\n",
        "    'vegetarian': ['veg', 'plant based', 'no meat'],\n",
        "    'vegan': ['vegan', 'dairy free', 'no animal'],\n",
        "    'gluten-free': ['gluten-free', 'gf', 'no gluten'],\n",
        "    'spicy': ['spicy', 'hot', 'chili'],\n",
        "}\n",
        "\n",
        "MENU_CATEGORIES: Dict[str, List[str]] = {\n",
        "    'appetizer': ['platter', 'starter', 'soup', 'salad', 'bruschetta'],\n",
        "    'main_course': ['curry', 'rice', 'noodles', 'burger', 'pizza', 'pasta'],\n",
        "    'dessert': ['ice cream', 'cake', 'sweet', 'pastry', 'pie'],\n",
        "    'beverage': ['juice', 'coffee', 'tea', 'smoothie'],\n",
        "}\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    filename=\"preprocessing.log\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s %(levelname)s:%(message)s\",\n",
        ")\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s]', '', text or \"\").lower().strip()\n",
        "\n",
        "def extract_dietary_tags(description: str) -> List[str]:\n",
        "    desc = normalize_text(description)\n",
        "    tags = {\n",
        "        tag for tag, keywords in DIETARY_KEYWORDS.items()\n",
        "        if any(kw in desc for kw in keywords)\n",
        "    }\n",
        "    return sorted(tags)\n",
        "\n",
        "def classify_menu_category(name: str, description: str) -> str:\n",
        "    text = f\"{normalize_text(name)} {normalize_text(description)}\"\n",
        "    for category, keywords in MENU_CATEGORIES.items():\n",
        "        if any(kw in text for kw in keywords):\n",
        "            return category\n",
        "    return 'other'\n",
        "\n",
        "def extract_ingredients(description: str) -> List[str]:\n",
        "    desc = description or \"\"\n",
        "    for pattern in INGREDIENTS_PATTERNS:\n",
        "        match = pattern.search(desc)\n",
        "        if match:\n",
        "            parts = re.split(r',\\s*| and ', match.group(1))\n",
        "            return [p.lower().strip() for p in parts if p.strip()]\n",
        "    return []\n",
        "\n",
        "def extract_spice_level(description: str) -> int:\n",
        "    if m := SPICE_LEVEL_PATTERN.search(description or \"\"):\n",
        "        return int(m.group(1))\n",
        "    return None  # Or 0, if you prefer\n",
        "\n",
        "def process_menu_items(data: Dict[str, Any]) -> None:\n",
        "    now_iso = datetime.now(timezone.utc).isoformat()\n",
        "    for item in data.get('menu', {}).get('hasMenuItem', []):\n",
        "        desc = item.get('description', '') or \"\"\n",
        "        name = item.get('name', '') or \"\"\n",
        "        item.update({\n",
        "            'dietary_tags': extract_dietary_tags(desc),\n",
        "            'menu_category': classify_menu_category(name, desc),\n",
        "            'normalized_name': normalize_text(name),\n",
        "            'ingredients': extract_ingredients(desc),\n",
        "            'spice_level': extract_spice_level(desc),\n",
        "            'processed_at': now_iso,\n",
        "        })\n",
        "\n",
        "def process_all_restaurants(output_dir: str = OUTPUT_DIR) -> None:\n",
        "    for rest in os.listdir(output_dir):\n",
        "        path = os.path.join(output_dir, rest, 'structured_data.json')\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            process_menu_items(data)\n",
        "\n",
        "            with open(path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            logging.info(f\"Processed {len(data['menu']['hasMenuItem'])} items for {rest}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed {rest}: {e}\", exc_info=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_all_restaurants()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LX3bGQLAnkNT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38K1Cq-CMRog"
      },
      "outputs": [],
      "source": [
        "from pymilvus import connections, utility\n",
        "MILVUS_URI = \"## Enter your uri\"\n",
        "TOKEN = \"## enter your token\"\n",
        "\n",
        "connections.connect(\"default\", uri=MILVUS_URI, token=TOKEN)\n",
        "collection_name = \"knowledge_base\"\n",
        "dim = 384  # Dimension of the embedding model\n",
        "\n",
        "# Check and reset collection if it already exists\n",
        "check_collection = utility.has_collection(collection_name)\n",
        "if check_collection:\n",
        "    utility.drop_collection(collection_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyREkjWzSX5y"
      },
      "outputs": [],
      "source": [
        "from pymilvus import FieldSchema, DataType, CollectionSchema, Collection\n",
        "fields = [\n",
        "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "    FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=10000),\n",
        "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
        "]\n",
        "schema = CollectionSchema(fields, description=\"Knowledge base embeddings\")\n",
        "collection = Collection(name=collection_name, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5XizTi3Stbu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# OCR for images\n",
        "def process_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Text extraction for HTML\n",
        "def process_html(html_path):\n",
        "    try:\n",
        "        with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            soup = BeautifulSoup(f.read(), \"html.parser\")\n",
        "            return soup.get_text(separator=\"\\n\", strip=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing HTML {html_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Folder processing function\n",
        "def process_folder(folder_path):\n",
        "    documents = []\n",
        "\n",
        "    for sub_folder in os.listdir(folder_path):\n",
        "        sub_folder_path = os.path.join(folder_path, sub_folder)\n",
        "        if not os.path.isdir(sub_folder_path):\n",
        "            continue\n",
        "\n",
        "        aggregated_content = []\n",
        "\n",
        "        for root, _, files in os.walk(sub_folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    if file.endswith((\".json\")):\n",
        "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                            content = f.read().strip()\n",
        "                            if content:\n",
        "                                aggregated_content.append(content)\n",
        "\n",
        "                    elif file.endswith(\".html\"):\n",
        "                        content = process_html(file_path)\n",
        "                        if content:\n",
        "                            aggregated_content.append(content)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "        if aggregated_content:\n",
        "            combined_content = \"\\n\".join(aggregated_content)\n",
        "            documents.append(Document(page_content=combined_content, metadata={\"source\": sub_folder}))\n",
        "\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "04OcIEWdS_6F",
        "outputId": "fef07ff1-637d-4db4-fda6-9ac168dd7ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.55)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.33)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.55)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.33)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community\n",
        "!pip install langchain_community\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings  # updated import\n",
        "\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "documents = process_folder(\"./zomato_scraped_data\")\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(documents)\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "contents = [chunk.page_content for chunk in chunks]\n",
        "embeddings = embedding_model.embed_documents(contents)\n",
        "\n",
        "collection.insert([contents, embeddings])\n",
        "collection.flush()\n",
        "\n",
        "# Create an index for efficient vector search\n",
        "index_params = {\"index_type\": \"AUTOINDEX\", \"metric_type\": \"IP\", \"params\": {}}\n",
        "collection.create_index(\"embedding\", index_params)\n",
        "collection.load()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
